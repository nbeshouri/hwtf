{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nbeshouri/Documents/Projects/Metis/Project 4\n"
     ]
    }
   ],
   "source": [
    "cd /Users/nbeshouri/Documents/Projects/Metis/Project\\ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hwmf import utils, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU, Input, Dense, TimeDistributed, RepeatVector, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq2seq_models(english_vocab_size, french_vocab_size, cell_state_size=128, \n",
    "                       embedding_size = 64, num_layers=2, use_encoder_embeddings=True, \n",
    "                       use_decoder_embeddings=True):\n",
    "    \"\"\"\n",
    "    Build and return three models used for sequence-to-sequence translation.\n",
    "    \n",
    "    These models roughly implement the system described by\n",
    "    [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215).\n",
    "    \n",
    "    Args:\n",
    "        english_vocab_size (int): The number of unique English words in the dataset\n",
    "            including any metatokens like \"<PAD>\".\n",
    "        french_vocab_size (int): The number of unique French words in the dataset \n",
    "            including any metatokens like \"<PAD>\".\n",
    "        cell_state_size (int): The number of dimensions in each cell's state.\n",
    "        embedding_size (int): The output size the embedding layers.\n",
    "        num_layers (Optional[int]): The number of layers of GRU cells used by the\n",
    "            encoder and decoder. Defaults to 2.\n",
    "        use_encoder_embeddings (Optional[bool]): Whether or not use an embedding layer\n",
    "            on the model's encoder. Defaults to True.\n",
    "        use_decoder_embeddings (Optional[bool]): Whether or not use an embedding layer\n",
    "            on the model's decoder. This makes the model train more slowly but\n",
    "            dramatically increases validation accuracy.\n",
    "    \n",
    "    Returns: \n",
    "        A tuple of three Keras models which share parameters: \n",
    "            \n",
    "            1. train_model: This model is used to train all the shared parameters. \n",
    "               It has two inputs:\n",
    "               \n",
    "                   1. x_encoder_train: This is just the training data, and \n",
    "                      should have shape `(num_sequences, input_sequence_len)` \n",
    "                      when using embeddings.\n",
    "                   2. x_decoder_train: This is a version of `y_train` that has\n",
    "                      been shifted one time-step to the right, and had the\n",
    "                      \"<START>\" token inserted at the front. It should have\n",
    "                      the same shape as `x_encoder_train`.\n",
    "                      \n",
    "               This model's output is only used by the optimizerâ€”use the other\n",
    "               two models for actual prediction.\n",
    "            \n",
    "            2. pred_encoder_model: This model is used during prediction to\n",
    "               encode an English sentence for decoding. It takes a single\n",
    "               input with shape `(num_sequences, input_sequence_length)`  \n",
    "               assuming embeddings are used and returns a list of outputs, \n",
    "               one for each GRU layer, each with the shape `(num_sequences, \n",
    "               cell_state_size)`. \n",
    "            \n",
    "            3. pred_decoder_model: This model is essentially a language model\n",
    "               which, given the previously translated word and the current state \n",
    "               for each of its GRU layers, outputs the probabilities for the\n",
    "               next word in the translation. Used this way, the first element in\n",
    "               its input list should have shape `(num_sequences, 1)`, and should \n",
    "               be followed `num_layers` cell states, each with shape `(num_sequences, \n",
    "               cell_state_size)`. You could also feed in a whole translation and\n",
    "               use the generated probabilities to calculate its total likelihood.     \n",
    "    \n",
    "    \"\"\"\n",
    "    # Setup encoder input.\n",
    "    if use_encoder_embeddings:\n",
    "        encoder_input = Input(shape=(None,), name='encoder_input')\n",
    "        encoder_embedding_layer = Embedding(input_dim=english_vocab_size, \n",
    "            output_dim=embedding_size, mask_zero=True, name='encoder_embedding')\n",
    "        embedded_encoder_input = encoder_embedding_layer(encoder_input)\n",
    "    else:\n",
    "        encoder_input = Input(shape=(None, english_vocab_size), name='encoder_input')\n",
    "    encoder_gru_layers = [GRU(cell_state_size, return_sequences=True, return_state=True, \n",
    "        name=f'encoder_layer_{layer_num + 1}') for layer_num in range(num_layers - 1)]\n",
    "    encoder_gru_layers.append(GRU(cell_state_size, return_state=True, \n",
    "                                  name=f'encoder_layer_{num_layers}'))\n",
    "        \n",
    "    # Setup decoder input.\n",
    "    if use_decoder_embeddings:\n",
    "        decoder_input = Input(shape=(None,), name='decoder_input')\n",
    "        decoder_embedding_layer = Embedding(input_dim=french_vocab_size, \n",
    "            output_dim=embedding_size, mask_zero=True, name='decoder_embedding')\n",
    "        embedded_decoder_input = decoder_embedding_layer(decoder_input)\n",
    "    else:\n",
    "        decoder_input = Input(shape=(None, french_vocab_size), name='decoder_input')\n",
    "    \n",
    "    decoder_gru_layers = [GRU(cell_state_size, return_sequences=True, return_state=True, \n",
    "        name=f'decoder_layer_{layer_num + 1}') for layer_num in range(num_layers)]\n",
    "    \n",
    "    # Setup final dense layer.\n",
    "    dense = Dense(french_vocab_size, activation='softmax')\n",
    "    dense_distrib = TimeDistributed(dense)\n",
    "    \n",
    "    # Connect encoder and decoder layers.\n",
    "    temp_encoder_input = embedded_encoder_input if use_encoder_embeddings else encoder_input\n",
    "    temp_decoder_input = embedded_decoder_input if use_decoder_embeddings else decoder_input\n",
    "    encoder_states = []\n",
    "    for encoder_layer, decoder_layer in zip(encoder_gru_layers, decoder_gru_layers):\n",
    "        temp_encoder_output, temp_encoder_state = encoder_layer(temp_encoder_input)\n",
    "        temp_encoder_input = temp_encoder_output\n",
    "        temp_decoder_output, temp_decoder_state = decoder_layer(temp_decoder_input, \n",
    "            initial_state=temp_encoder_state)\n",
    "        temp_decoder_input = temp_decoder_output\n",
    "        encoder_states.append(temp_encoder_state)\n",
    "    \n",
    "    # Build the training model.\n",
    "    output = dense_distrib(temp_decoder_output)\n",
    "    train_model = Model([encoder_input, decoder_input], output)\n",
    "    train_model.compile(loss=sparse_categorical_crossentropy,\n",
    "                        optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Build prediction encoder model.\n",
    "    pred_encoder_model = Model(encoder_input, encoder_states)\n",
    "    \n",
    "    \n",
    "    # Build prediction decoder model.\n",
    "    pred_decoder_state_inputs = [Input(shape=(cell_state_size,), \n",
    "        name=f'decoder_layer_{layer_num + 1}_state_input') for layer_num in range(num_layers)]\n",
    "    \n",
    "    pred_decoder_states = []\n",
    "    temp_pred_decoder_input = embedded_decoder_input if use_decoder_embeddings else decoder_input   \n",
    "    for decoder_layer, decoder_state_input in zip(decoder_gru_layers, pred_decoder_state_inputs):\n",
    "        temp_pred_decoder_output, temp_pred_decoder_state = decoder_layer(\n",
    "            temp_pred_decoder_input, initial_state=decoder_state_input)\n",
    "        temp_pred_decoder_input = temp_pred_decoder_output\n",
    "        pred_decoder_states.append(temp_pred_decoder_state)\n",
    "    \n",
    "    pred_word_probs = dense(temp_pred_decoder_output)\n",
    "\n",
    "    pred_decoder_model = Model([decoder_input, *pred_decoder_state_inputs], \n",
    "                               [pred_word_probs, *pred_decoder_states])\n",
    "                               \n",
    "    return train_model, pred_encoder_model, pred_decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model, pred_encoder_model, pred_decoder_model = get_seq2seq_models(200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_string = \"\"\"Due to his banishment, he is hot-tempered, impatient and at times also depressive. He is shown to be caring and thoughtful, though his judgement was heavily clouded by his jealousy towards Azula. He has a close bond with his mother Ursa and his Uncle Iroh. He strives to regain honor and attention in his father's eyes by trying to capture the Avatar in his name. But after returning to the Fire Nation from his years of banishment, he realized his father's ill feelings towards him couldn't be fixed even by capturing the Avatar. Zuko decided that trying to regain honour, as a substitute for love, was worthless and a waste of time. He displayed aggression towards Aang when training him in fire-bending, because of the fact that it was imperitive Aang learn it before Sozin's Comet arrived at the end of the summer, though he was still being caring. He is shown to have a sarcastic side on several occasions. Due to his scar, he also has an inferiority complex about his appearance, stating angrily that: \"Normal teenagers worry about bad skin! I don't have that luxury! My father decided to teach me a permanent lesson -- on my face!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [big_string]\n",
    "word_to_vec, word_to_id, embedding_matrix = models.get_embeddings(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = models.get_nn_X(texts, word_to_id)\n",
    "y = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, None, 64)     6400        encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 64)     6400        decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer_1 (GRU)           [(None, None, 128),  74112       encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer_1 (GRU)           [(None, None, 128),  74112       decoder_embedding[0][0]          \n",
      "                                                                 encoder_layer_1[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer_2 (GRU)           [(None, 128), (None, 98688       encoder_layer_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer_2 (GRU)           [(None, None, 128),  98688       decoder_layer_1[0][0]            \n",
      "                                                                 encoder_layer_2[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 100)    12900       decoder_layer_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 371,300\n",
      "Trainable params: 371,300\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 14,  53,  91,  32,  21,  17,  36,  10,  85,  90,  82, 100,  23,\n",
       "         97,  17,  72,  53,  61, 121,  85,   1,  67,  91,  30,  15, 116,\n",
       "         75,  59,  91,  87]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.array([[0, 14,  53,  91,  32,  21,  17,  36,  10,  85,  90,  82, 100,  23,\n",
    "         97,  17,  72,  53,  61, 121,  85,   1,  67,  91,  30,  15, 116,\n",
    "         75,  59,  91]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected time_distributed_1 to have 3 dimensions, but got array with shape (1, 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-492cf652f7f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/aind/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/aind/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m~/anaconda/envs/aind/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected time_distributed_1 to have 3 dimensions, but got array with shape (1, 30)"
     ]
    }
   ],
   "source": [
    "train_model.fit([X, X2], X, batch_size=1, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 30\n",
    "recur_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 30, 200)           25800     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 30, 256)           252672    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 30, 128)           32896     \n",
      "=================================================================\n",
      "Total params: 311,368\n",
      "Trainable params: 311,368\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(max_sequence_length,))\n",
    "embeddings = Embedding(embedding_matrix.shape[0],\n",
    "                       embedding_matrix.shape[1],\n",
    "                       weights=[embedding_matrix],\n",
    "                       trainable=True)(inputs)\n",
    "\n",
    "encoded = Bidirectional(GRU(recur_size))(embeddings)\n",
    "\n",
    "RepeatVector(max_sequence_length)(encoded)\n",
    "\n",
    "decoded = Bidirectional(GRU(recur_size, return_sequences=True))(embeddings)\n",
    "\n",
    "outputs = TimeDistributed(Dense(128))(decoded)\n",
    "\n",
    "# outputs = Dense(num_classes, activation='softmax')(dense)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
